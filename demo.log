Using device: cpu
========= Configuration ==========
DEVICE: cpu
SCALE_FACTOR: 1
WIDTH: 512
MORPH_REUSE_DIM: 128
MORPH_REUSE_EXPAND: 2
EPOCHS: 5
BATCH_SIZE: 128
LR_BASELINE: 0.001
LR_MORPH_REUSE: 0.001
PRETRAINED_NAME: huawei-noah/TinyBERT_General_4L_312D
LLM_SEQ_LEN: 64
MAX_SAMPLES: 1000
BATCH_SIZE_LM: 32
DATASETS: {'MNIST': {'type': 'classification', 'num_classes': 10, 'input_shape': (1, 28, 28), 'transform': Compose(
    ToTensor()
    Normalize(mean=(0.5,), std=(0.5,))
)}, 'FashionMNIST': {'type': 'classification', 'num_classes': 10, 'input_shape': (1, 28, 28), 'transform': Compose(
    ToTensor()
    Normalize(mean=(0.5,), std=(0.5,))
)}, 'CIFAR10': {'type': 'classification', 'num_classes': 10, 'input_shape': (3, 32, 32), 'transform': Compose(
    ToTensor()
    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
)}, 'sst2': {'type': 'llm-classification', 'num_classes': 2, 'input_shape': (64,), 'transform': None}}
=================================

========= System Information =========
OS: Linux-6.1.123+-x86_64-with-glibc2.35
CPU: AMD EPYC 7B12 (2Ã—2.25 GHz)
RAM: 12977 MB
Python: 3.11.13
PyTorch: 2.3.1+cpu
Device: cpu
=====================================


==================================================
Running experiment: MNIST
==================================================

Dataset: MNIST
Type: Classification
Classes: 10
Input shape: (1, 28, 28)
Train samples: 60000
Test samples: 10000
Batch size: 128
Building Vision Baseline...
Building Vision MorphReuse...
Building Vision LoRA...

Model Parameters:
Baseline:
  Total params: 669,706
  Trainable params: 669,706
  Percentage trainable: 100.00%
MorphReuse:
  Total params: 298,625
  Trainable params: 65,921
  Percentage trainable: 22.07%
LoRA:
  Total params: 681,074
  Trainable params: 11,368
  Percentage trainable: 1.67%

Training Baseline:

MNIST-Baseline (Initial) Memory (MB)
  Params:   2.55
  Grad:     2.55
  Buffers:  0.00
  Optim:    5.11
  Total:    10.22
[MNIST-Baseline] Evaluating initial accuracy...
  Initial Train: 8.42%, Test: 7.96%
[MNIST-Baseline] Epoch 1/5: Train: 95.81%, Test: 95.43% | Time: Fwd 1.9s, Bwd 2.0s, Upd 1.7s Total 5.6s
[MNIST-Baseline] Epoch 2/5: Train: 96.92%, Test: 96.43% | Time: Fwd 1.9s, Bwd 2.0s, Upd 1.7s Total 5.6s
[MNIST-Baseline] Epoch 3/5: Train: 98.00%, Test: 97.21% | Time: Fwd 1.8s, Bwd 1.9s, Upd 1.6s Total 5.3s
[MNIST-Baseline] Epoch 4/5: Train: 98.04%, Test: 96.93% | Time: Fwd 1.8s, Bwd 1.9s, Upd 1.6s Total 5.3s
[MNIST-Baseline] Epoch 5/5: Train: 97.51%, Test: 96.63% | Time: Fwd 1.8s, Bwd 1.9s, Upd 1.7s Total 5.4s

MNIST-Baseline (Final) Memory (MB)
  Params:   2.55
  Grad:     2.55
  Buffers:  0.00
  Optim:    5.11
  Total:    10.22

Training MorphReuse:

MNIST-MorphReuse (Initial) Memory (MB)
  Params:   1.14
  Grad:     0.25
  Buffers:  0.00
  Optim:    0.50
  Total:    1.89
[MNIST-MorphReuse] Evaluating initial accuracy...
  Initial Train: 9.34%, Test: 9.90%
[MNIST-MorphReuse] Epoch 1/5: Train: 92.33%, Test: 92.39% | Time: Fwd 1.7s, Bwd 1.3s, Upd 0.4s Total 3.4s
[MNIST-MorphReuse] Epoch 2/5: Train: 95.81%, Test: 95.40% | Time: Fwd 1.7s, Bwd 1.3s, Upd 0.4s Total 3.4s
[MNIST-MorphReuse] Epoch 3/5: Train: 96.69%, Test: 95.92% | Time: Fwd 1.6s, Bwd 1.2s, Upd 0.4s Total 3.3s
[MNIST-MorphReuse] Epoch 4/5: Train: 97.13%, Test: 96.01% | Time: Fwd 1.6s, Bwd 1.2s, Upd 0.4s Total 3.2s
[MNIST-MorphReuse] Epoch 5/5: Train: 97.68%, Test: 96.72% | Time: Fwd 1.5s, Bwd 1.2s, Upd 0.4s Total 3.1s

MNIST-MorphReuse (Final) Memory (MB)
  Params:   1.14
  Grad:     0.25
  Buffers:  0.00
  Optim:    0.50
  Total:    1.89

Training LoRA:

MNIST-LoRA (Initial) Memory (MB)
  Params:   2.60
  Grad:     0.04
  Buffers:  0.00
  Optim:    0.09
  Total:    2.73
[MNIST-LoRA] Evaluating initial accuracy...
  Initial Train: 8.99%, Test: 9.44%
[MNIST-LoRA] Epoch 1/5: Train: 80.88%, Test: 81.26% | Time: Fwd 2.1s, Bwd 1.0s, Upd 0.4s Total 3.5s
[MNIST-LoRA] Epoch 2/5: Train: 86.75%, Test: 86.59% | Time: Fwd 2.2s, Bwd 1.1s, Upd 0.4s Total 3.6s
[MNIST-LoRA] Epoch 3/5: Train: 89.42%, Test: 89.08% | Time: Fwd 2.3s, Bwd 1.1s, Upd 0.4s Total 3.8s
[MNIST-LoRA] Epoch 4/5: Train: 91.12%, Test: 90.75% | Time: Fwd 2.2s, Bwd 1.1s, Upd 0.4s Total 3.6s
[MNIST-LoRA] Epoch 5/5: Train: 92.07%, Test: 91.74% | Time: Fwd 2.1s, Bwd 1.0s, Upd 0.4s Total 3.5s

MNIST-LoRA (Final) Memory (MB)
  Params:   2.60
  Grad:     0.04
  Buffers:  0.00
  Optim:    0.09
  Total:    2.73


==================================================
Running experiment: FashionMNIST
==================================================

Dataset: FashionMNIST
Type: Classification
Classes: 10
Input shape: (1, 28, 28)
Train samples: 60000
Test samples: 10000
Batch size: 128
Building Vision Baseline...
Building Vision MorphReuse...
Building Vision LoRA...

Model Parameters:
Baseline:
  Total params: 669,706
  Trainable params: 669,706
  Percentage trainable: 100.00%
MorphReuse:
  Total params: 298,625
  Trainable params: 65,921
  Percentage trainable: 22.07%
LoRA:
  Total params: 681,074
  Trainable params: 11,368
  Percentage trainable: 1.67%

Training Baseline:

FashionMNIST-Baseline (Initial) Memory (MB)
  Params:   2.55
  Grad:     2.55
  Buffers:  0.00
  Optim:    5.11
  Total:    10.22
[FashionMNIST-Baseline] Evaluating initial accuracy...
  Initial Train: 4.59%, Test: 4.68%
[FashionMNIST-Baseline] Epoch 1/5: Train: 85.83%, Test: 83.90% | Time: Fwd 1.8s, Bwd 2.0s, Upd 1.6s Total 5.4s
[FashionMNIST-Baseline] Epoch 2/5: Train: 88.12%, Test: 86.22% | Time: Fwd 1.8s, Bwd 1.9s, Upd 1.6s Total 5.3s
[FashionMNIST-Baseline] Epoch 3/5: Train: 89.34%, Test: 86.99% | Time: Fwd 1.8s, Bwd 1.9s, Upd 1.6s Total 5.3s
[FashionMNIST-Baseline] Epoch 4/5: Train: 89.84%, Test: 87.32% | Time: Fwd 1.7s, Bwd 1.9s, Upd 1.6s Total 5.2s
[FashionMNIST-Baseline] Epoch 5/5: Train: 90.95%, Test: 87.91% | Time: Fwd 1.8s, Bwd 1.9s, Upd 1.6s Total 5.3s

FashionMNIST-Baseline (Final) Memory (MB)
  Params:   2.55
  Grad:     2.55
  Buffers:  0.00
  Optim:    5.11
  Total:    10.22

Training MorphReuse:

FashionMNIST-MorphReuse (Initial) Memory (MB)
  Params:   1.14
  Grad:     0.25
  Buffers:  0.00
  Optim:    0.50
  Total:    1.89
[FashionMNIST-MorphReuse] Evaluating initial accuracy...
  Initial Train: 11.60%, Test: 11.47%
[FashionMNIST-MorphReuse] Epoch 1/5: Train: 84.55%, Test: 83.30% | Time: Fwd 1.6s, Bwd 1.3s, Upd 0.4s Total 3.3s
[FashionMNIST-MorphReuse] Epoch 2/5: Train: 85.74%, Test: 84.38% | Time: Fwd 1.6s, Bwd 1.3s, Upd 0.4s Total 3.3s
[FashionMNIST-MorphReuse] Epoch 3/5: Train: 87.02%, Test: 85.57% | Time: Fwd 1.6s, Bwd 1.2s, Upd 0.4s Total 3.2s
[FashionMNIST-MorphReuse] Epoch 4/5: Train: 88.04%, Test: 85.80% | Time: Fwd 1.5s, Bwd 1.2s, Upd 0.4s Total 3.1s
[FashionMNIST-MorphReuse] Epoch 5/5: Train: 88.53%, Test: 86.16% | Time: Fwd 1.6s, Bwd 1.3s, Upd 0.4s Total 3.3s

FashionMNIST-MorphReuse (Final) Memory (MB)
  Params:   1.14
  Grad:     0.25
  Buffers:  0.00
  Optim:    0.50
  Total:    1.89

Training LoRA:

FashionMNIST-LoRA (Initial) Memory (MB)
  Params:   2.60
  Grad:     0.04
  Buffers:  0.00
  Optim:    0.09
  Total:    2.73
[FashionMNIST-LoRA] Evaluating initial accuracy...
  Initial Train: 14.32%, Test: 15.13%
[FashionMNIST-LoRA] Epoch 1/5: Train: 78.57%, Test: 77.55% | Time: Fwd 2.3s, Bwd 1.1s, Upd 0.4s Total 3.8s
[FashionMNIST-LoRA] Epoch 2/5: Train: 81.36%, Test: 80.16% | Time: Fwd 2.2s, Bwd 1.1s, Upd 0.4s Total 3.7s
[FashionMNIST-LoRA] Epoch 3/5: Train: 83.37%, Test: 81.91% | Time: Fwd 2.2s, Bwd 1.1s, Upd 0.4s Total 3.6s
[FashionMNIST-LoRA] Epoch 4/5: Train: 83.78%, Test: 82.29% | Time: Fwd 2.1s, Bwd 1.1s, Upd 0.3s Total 3.5s
[FashionMNIST-LoRA] Epoch 5/5: Train: 84.94%, Test: 83.44% | Time: Fwd 2.2s, Bwd 1.1s, Upd 0.4s Total 3.6s

FashionMNIST-LoRA (Final) Memory (MB)
  Params:   2.60
  Grad:     0.04
  Buffers:  0.00
  Optim:    0.09
  Total:    2.73


==================================================
Running experiment: CIFAR10
==================================================
Files already downloaded and verified
Files already downloaded and verified

Dataset: CIFAR10
Type: Classification
Classes: 10
Input shape: (3, 32, 32)
Train samples: 50000
Test samples: 10000
Batch size: 128
Building Vision Baseline...
Building Vision MorphReuse...
Building Vision LoRA...

Model Parameters:
Baseline:
  Total params: 1,841,162
  Trainable params: 1,841,162
  Percentage trainable: 100.00%
MorphReuse:
  Total params: 591,489
  Trainable params: 65,921
  Percentage trainable: 11.14%
LoRA:
  Total params: 1,861,682
  Trainable params: 20,520
  Percentage trainable: 1.10%

Training Baseline:

CIFAR10-Baseline (Initial) Memory (MB)
  Params:   7.02
  Grad:     7.02
  Buffers:  0.00
  Optim:    14.05
  Total:    28.09
[CIFAR10-Baseline] Evaluating initial accuracy...
  Initial Train: 9.96%, Test: 9.48%
[CIFAR10-Baseline] Epoch 1/5: Train: 49.64%, Test: 47.55% | Time: Fwd 3.2s, Bwd 3.4s, Upd 3.5s Total 10.2s
[CIFAR10-Baseline] Epoch 2/5: Train: 53.15%, Test: 49.11% | Time: Fwd 3.2s, Bwd 3.4s, Upd 3.5s Total 10.1s
[CIFAR10-Baseline] Epoch 3/5: Train: 58.71%, Test: 51.86% | Time: Fwd 3.1s, Bwd 3.4s, Upd 3.4s Total 9.9s
[CIFAR10-Baseline] Epoch 4/5: Train: 62.65%, Test: 53.84% | Time: Fwd 3.2s, Bwd 3.5s, Upd 3.6s Total 10.3s
[CIFAR10-Baseline] Epoch 5/5: Train: 64.63%, Test: 52.82% | Time: Fwd 3.2s, Bwd 3.5s, Upd 3.5s Total 10.2s

CIFAR10-Baseline (Final) Memory (MB)
  Params:   7.02
  Grad:     7.02
  Buffers:  0.00
  Optim:    14.05
  Total:    28.09

Training MorphReuse:

CIFAR10-MorphReuse (Initial) Memory (MB)
  Params:   2.26
  Grad:     0.25
  Buffers:  0.00
  Optim:    0.50
  Total:    3.01
[CIFAR10-MorphReuse] Evaluating initial accuracy...
  Initial Train: 11.45%, Test: 11.75%
[CIFAR10-MorphReuse] Epoch 1/5: Train: 43.67%, Test: 43.08% | Time: Fwd 1.8s, Bwd 1.0s, Upd 0.3s Total 3.1s
[CIFAR10-MorphReuse] Epoch 2/5: Train: 47.59%, Test: 45.79% | Time: Fwd 1.8s, Bwd 1.0s, Upd 0.4s Total 3.2s
[CIFAR10-MorphReuse] Epoch 3/5: Train: 49.88%, Test: 47.03% | Time: Fwd 1.8s, Bwd 1.0s, Upd 0.4s Total 3.2s
[CIFAR10-MorphReuse] Epoch 4/5: Train: 51.57%, Test: 48.33% | Time: Fwd 1.8s, Bwd 1.0s, Upd 0.3s Total 3.1s
[CIFAR10-MorphReuse] Epoch 5/5: Train: 53.76%, Test: 49.03% | Time: Fwd 1.9s, Bwd 1.1s, Upd 0.4s Total 3.4s

CIFAR10-MorphReuse (Final) Memory (MB)
  Params:   2.26
  Grad:     0.25
  Buffers:  0.00
  Optim:    0.50
  Total:    3.01

Training LoRA:

CIFAR10-LoRA (Initial) Memory (MB)
  Params:   7.10
  Grad:     0.08
  Buffers:  0.00
  Optim:    0.16
  Total:    7.34
[CIFAR10-LoRA] Evaluating initial accuracy...
  Initial Train: 10.37%, Test: 10.34%
[CIFAR10-LoRA] Epoch 1/5: Train: 35.86%, Test: 35.82% | Time: Fwd 3.6s, Bwd 1.0s, Upd 0.3s Total 4.9s
[CIFAR10-LoRA] Epoch 2/5: Train: 38.39%, Test: 38.03% | Time: Fwd 3.6s, Bwd 1.0s, Upd 0.3s Total 5.0s
[CIFAR10-LoRA] Epoch 3/5: Train: 39.81%, Test: 39.41% | Time: Fwd 3.5s, Bwd 1.0s, Upd 0.3s Total 4.8s
[CIFAR10-LoRA] Epoch 4/5: Train: 41.23%, Test: 40.61% | Time: Fwd 3.5s, Bwd 1.0s, Upd 0.3s Total 4.9s
[CIFAR10-LoRA] Epoch 5/5: Train: 42.04%, Test: 41.05% | Time: Fwd 3.5s, Bwd 1.0s, Upd 0.3s Total 4.8s

CIFAR10-LoRA (Final) Memory (MB)
  Params:   7.10
  Grad:     0.08
  Buffers:  0.00
  Optim:    0.16
  Total:    7.34


==================================================
Running experiment: sst2
==================================================
Labels counts: Counter({1: 553, 0: 447})
Labels counts: Counter({1: 444, 0: 428})

Dataset: sst2
Type: Text Classification
Classes: 2
Sequence length: 64
Train samples: 1000
Test samples: 872
Batch size: 32
Building LLM Baseline...
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Building LLM MorphReuse...
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Shared Weights:
 {(312, 312): 8, (312, 1200): 2, (1200, 312): 2, (312, 2): 1}
Building LLM LoRA...
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Model Parameters:
Baseline:
  Total params: 14,350,874
  Trainable params: 2,284,994
  Percentage trainable: 15.92%
MorphReuse:
  Total params: 12,916,983
  Trainable params: 851,103
  Percentage trainable: 6.59%
LoRA:
  Total params: 14,439,820
  Trainable params: 88,946
  Percentage trainable: 0.62%

Training Baseline:

sst2-Baseline (Initial) Memory (MB)
  Params:   54.74
  Grad:     8.72
  Buffers:  0.01
  Optim:    17.43
  Total:    80.90
[sst2-Baseline] Evaluating initial accuracy...
  Initial Train: 56.10%, Test: 56.08%
[sst2-Baseline] Epoch 1/5: Train: 81.40%, Test: 77.52% | Time: Fwd 11.8s, Bwd 9.0s, Upd 0.3s Total 21.1s
[sst2-Baseline] Epoch 2/5: Train: 83.70%, Test: 79.01% | Time: Fwd 11.9s, Bwd 9.1s, Upd 0.3s Total 21.4s
[sst2-Baseline] Epoch 3/5: Train: 86.20%, Test: 81.77% | Time: Fwd 11.6s, Bwd 9.1s, Upd 0.3s Total 21.0s
[sst2-Baseline] Epoch 4/5: Train: 81.60%, Test: 73.74% | Time: Fwd 15.2s, Bwd 11.8s, Upd 0.4s Total 27.4s
[sst2-Baseline] Epoch 5/5: Train: 90.70%, Test: 83.83% | Time: Fwd 13.1s, Bwd 9.9s, Upd 0.4s Total 23.4s

sst2-Baseline (Final) Memory (MB)
  Params:   54.74
  Grad:     8.72
  Buffers:  0.01
  Optim:    17.43
  Total:    80.90

Training MorphReuse:

sst2-MorphReuse (Initial) Memory (MB)
  Params:   49.27
  Grad:     3.25
  Buffers:  0.01
  Optim:    6.49
  Total:    59.02
[sst2-MorphReuse] Evaluating initial accuracy...
  Initial Train: 56.60%, Test: 53.33%
[sst2-MorphReuse] Epoch 1/5: Train: 55.30%, Test: 50.92% | Time: Fwd 12.3s, Bwd 9.9s, Upd 0.2s Total 22.4s
[sst2-MorphReuse] Epoch 2/5: Train: 71.90%, Test: 71.56% | Time: Fwd 16.3s, Bwd 13.0s, Upd 0.2s Total 29.5s
[sst2-MorphReuse] Epoch 3/5: Train: 81.10%, Test: 78.90% | Time: Fwd 16.8s, Bwd 13.0s, Upd 0.3s Total 30.1s
[sst2-MorphReuse] Epoch 4/5: Train: 84.90%, Test: 80.28% | Time: Fwd 12.2s, Bwd 9.8s, Upd 0.2s Total 22.2s
[sst2-MorphReuse] Epoch 5/5: Train: 86.00%, Test: 80.96% | Time: Fwd 12.4s, Bwd 10.0s, Upd 0.2s Total 22.5s

sst2-MorphReuse (Final) Memory (MB)
  Params:   49.27
  Grad:     3.25
  Buffers:  0.01
  Optim:    6.49
  Total:    59.02

Training LoRA:

sst2-LoRA (Initial) Memory (MB)
  Params:   55.08
  Grad:     0.34
  Buffers:  0.01
  Optim:    0.68
  Total:    56.11
[sst2-LoRA] Evaluating initial accuracy...
  Initial Train: 55.30%, Test: 50.92%
[sst2-LoRA] Epoch 1/5: Train: 78.00%, Test: 74.43% | Time: Fwd 14.2s, Bwd 17.1s, Upd 0.1s Total 31.3s
[sst2-LoRA] Epoch 2/5: Train: 84.80%, Test: 81.54% | Time: Fwd 14.1s, Bwd 17.3s, Upd 0.1s Total 31.5s
[sst2-LoRA] Epoch 3/5: Train: 87.10%, Test: 83.26% | Time: Fwd 14.3s, Bwd 17.2s, Upd 0.1s Total 31.6s
[sst2-LoRA] Epoch 4/5: Train: 86.50%, Test: 78.21% | Time: Fwd 14.4s, Bwd 17.5s, Upd 0.1s Total 32.1s
[sst2-LoRA] Epoch 5/5: Train: 91.30%, Test: 83.72% | Time: Fwd 14.0s, Bwd 16.9s, Upd 0.1s Total 31.0s

sst2-LoRA (Final) Memory (MB)
  Params:   55.08
  Grad:     0.34
  Buffers:  0.01
  Optim:    0.68
  Total:    56.11
