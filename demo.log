Using device: cpu
========= Configuration ==========
DEVICE: cpu
WIDTH: 512
MORPH_REUSE_DIM: 128
MORPH_REUSE_EXPAND: 2.5
EPOCHS: 5
BATCH_SIZE: 128
LR_VISION: 0.001
PRETRAINED_NAME: huawei-noah/TinyBERT_General_4L_312D
LLM_SEQ_LEN: 64
MAX_SAMPLES: 1000
BATCH_SIZE_LM: 32
LR_LM: 0.001
DATASETS: {'MNIST': {'type': 'classification', 'num_classes': 10, 'input_shape': (1, 28, 28), 'transform': Compose(
    ToTensor()
    Normalize(mean=(0.5,), std=(0.5,))
)}, 'FashionMNIST': {'type': 'classification', 'num_classes': 10, 'input_shape': (1, 28, 28), 'transform': Compose(
    ToTensor()
    Normalize(mean=(0.5,), std=(0.5,))
)}, 'CIFAR10': {'type': 'classification', 'num_classes': 10, 'input_shape': (3, 32, 32), 'transform': Compose(
    ToTensor()
    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
)}, 'sst2': {'type': 'llm-classification', 'num_classes': 2, 'input_shape': (64,), 'transform': None}}
=================================

========= System Information =========
OS: Linux-6.1.123+-x86_64-with-glibc2.35
CPU: Intel(R) Xeon(R) CPU @ 2.20GHz (2Ã—2.20 GHz)
RAM: 12977 MB
Python: 3.11.13
PyTorch: 2.6.0+cu124
Device: cpu
=====================================


==================================================
Running experiment: sst2
==================================================
Labels counts: Counter({1: 521, 0: 479})
Labels counts: Counter({1: 444, 0: 428})

Dataset: sst2
Type: Text Classification
Classes: 2
Sequence length: 64
Train samples: 1000
Test samples: 872
Batch size: 32
Building LLM Baseline...
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Building LLM MorphReuse...
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Shared Weights:
 {(312, 312): 8, (312, 1200): 2, (1200, 312): 2, (312, 2): 1}
Building LLM LoRA...
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Model Parameters:
Baseline:
  Total params: 14,350,874
  Trainable params: 2,284,994
  Percentage trainable: 15.92%
MorphReuse:
  Total params: 12,916,983
  Trainable params: 851,103
  Percentage trainable: 6.59%
LoRA:
  Total params: 14,439,820
  Trainable params: 88,946
  Percentage trainable: 0.62%

Training Baseline:

sst2-Baseline (Initial) Memory (MB)
  Params:   54.74
  Grad:     8.72
  Buffers:  0.01
  Optim:    17.43
  Total:    80.90
[sst2-Baseline] Evaluating initial accuracy...
  Initial Train: 52.20%, Test: 50.92%
[sst2-Baseline] Epoch 1/5: Train: 80.80%, Test: 77.75% | Time: Fwd 14.8s, Bwd 10.9s, Upd 0.4s Total 26.2s
[sst2-Baseline] Epoch 2/5: Train: 78.80%, Test: 72.82% | Time: Fwd 15.0s, Bwd 10.9s, Upd 0.4s Total 26.3s
[sst2-Baseline] Epoch 3/5: Train: 81.30%, Test: 79.01% | Time: Fwd 15.2s, Bwd 11.1s, Upd 0.4s Total 26.8s
[sst2-Baseline] Epoch 4/5: Train: 83.80%, Test: 82.34% | Time: Fwd 14.7s, Bwd 11.0s, Upd 0.4s Total 26.2s
[sst2-Baseline] Epoch 5/5: Train: 86.40%, Test: 81.54% | Time: Fwd 14.8s, Bwd 11.0s, Upd 0.4s Total 26.2s

sst2-Baseline (Final) Memory (MB)
  Params:   54.74
  Grad:     8.72
  Buffers:  0.01
  Optim:    17.43
  Total:    80.90

Training MorphReuse:

sst2-MorphReuse (Initial) Memory (MB)
  Params:   49.27
  Grad:     3.25
  Buffers:  0.01
  Optim:    6.49
  Total:    59.02
[sst2-MorphReuse] Evaluating initial accuracy...
  Initial Train: 47.50%, Test: 48.62%
[sst2-MorphReuse] Epoch 1/5: Train: 75.30%, Test: 69.95% | Time: Fwd 15.7s, Bwd 12.9s, Upd 0.2s Total 28.8s
[sst2-MorphReuse] Epoch 2/5: Train: 82.10%, Test: 78.78% | Time: Fwd 16.0s, Bwd 13.1s, Upd 0.2s Total 29.3s
[sst2-MorphReuse] Epoch 3/5: Train: 85.10%, Test: 82.11% | Time: Fwd 16.1s, Bwd 13.1s, Upd 0.2s Total 29.5s
[sst2-MorphReuse] Epoch 4/5: Train: 86.10%, Test: 81.65% | Time: Fwd 15.7s, Bwd 12.8s, Upd 0.2s Total 28.7s
[sst2-MorphReuse] Epoch 5/5: Train: 84.50%, Test: 80.28% | Time: Fwd 15.6s, Bwd 12.8s, Upd 0.2s Total 28.6s

sst2-MorphReuse (Final) Memory (MB)
  Params:   49.27
  Grad:     3.25
  Buffers:  0.01
  Optim:    6.49
  Total:    59.02

Training LoRA:

sst2-LoRA (Initial) Memory (MB)
  Params:   55.08
  Grad:     0.34
  Buffers:  0.01
  Optim:    0.68
  Total:    56.11
[sst2-LoRA] Evaluating initial accuracy...
  Initial Train: 47.90%, Test: 49.08%
[sst2-LoRA] Epoch 1/5: Train: 81.20%, Test: 79.24% | Time: Fwd 17.8s, Bwd 15.6s, Upd 0.1s Total 33.5s
[sst2-LoRA] Epoch 2/5: Train: 80.70%, Test: 76.38% | Time: Fwd 17.7s, Bwd 15.6s, Upd 0.1s Total 33.4s
[sst2-LoRA] Epoch 3/5: Train: 87.90%, Test: 82.91% | Time: Fwd 17.9s, Bwd 16.3s, Upd 0.1s Total 34.4s
[sst2-LoRA] Epoch 4/5: Train: 88.90%, Test: 84.98% | Time: Fwd 17.3s, Bwd 15.2s, Upd 0.1s Total 32.6s
[sst2-LoRA] Epoch 5/5: Train: 92.00%, Test: 82.11% | Time: Fwd 17.6s, Bwd 15.4s, Upd 0.1s Total 33.2s

sst2-LoRA (Final) Memory (MB)
  Params:   55.08
  Grad:     0.34
  Buffers:  0.01
  Optim:    0.68
  Total:    56.11


==================================================
Running experiment: MNIST
==================================================

Dataset: MNIST
Type: Classification
Classes: 10
Input shape: (1, 28, 28)
Train samples: 60000
Test samples: 10000
Batch size: 128
Building Vision Baseline...
Building Vision MorphReuse...
Building Vision LoRA...

Model Parameters:
Baseline:
  Total params: 669,706
  Trainable params: 669,706
  Percentage trainable: 100.00%
MorphReuse:
  Total params: 315,713
  Trainable params: 83,009
  Percentage trainable: 26.29%
LoRA:
  Total params: 681,074
  Trainable params: 11,368
  Percentage trainable: 1.67%

Training Baseline:

MNIST-Baseline (Initial) Memory (MB)
  Params:   2.55
  Grad:     2.55
  Buffers:  0.00
  Optim:    5.11
  Total:    10.22
[MNIST-Baseline] Evaluating initial accuracy...
  Initial Train: 6.54%, Test: 6.20%
[MNIST-Baseline] Epoch 1/5: Train: 94.70%, Test: 94.84% | Time: Fwd 1.8s, Bwd 2.2s, Upd 2.2s Total 6.2s
[MNIST-Baseline] Epoch 2/5: Train: 96.31%, Test: 95.76% | Time: Fwd 1.8s, Bwd 2.2s, Upd 4.3s Total 8.3s
[MNIST-Baseline] Epoch 3/5: Train: 97.88%, Test: 97.17% | Time: Fwd 1.8s, Bwd 2.3s, Upd 5.0s Total 9.1s
[MNIST-Baseline] Epoch 4/5: Train: 98.19%, Test: 97.08% | Time: Fwd 1.8s, Bwd 2.2s, Upd 4.9s Total 8.9s
[MNIST-Baseline] Epoch 5/5: Train: 98.01%, Test: 97.15% | Time: Fwd 1.8s, Bwd 2.2s, Upd 4.9s Total 9.0s

MNIST-Baseline (Final) Memory (MB)
  Params:   2.55
  Grad:     2.55
  Buffers:  0.00
  Optim:    5.11
  Total:    10.22

Training MorphReuse:

MNIST-MorphReuse (Initial) Memory (MB)
  Params:   1.20
  Grad:     0.32
  Buffers:  0.00
  Optim:    0.63
  Total:    2.15
[MNIST-MorphReuse] Evaluating initial accuracy...
  Initial Train: 11.43%, Test: 11.21%
[MNIST-MorphReuse] Epoch 1/5: Train: 93.60%, Test: 93.40% | Time: Fwd 1.7s, Bwd 1.7s, Upd 0.6s Total 4.0s
[MNIST-MorphReuse] Epoch 2/5: Train: 95.73%, Test: 95.61% | Time: Fwd 1.7s, Bwd 1.8s, Upd 0.6s Total 4.1s
[MNIST-MorphReuse] Epoch 3/5: Train: 95.52%, Test: 94.95% | Time: Fwd 1.7s, Bwd 1.8s, Upd 0.6s Total 4.1s
[MNIST-MorphReuse] Epoch 4/5: Train: 96.82%, Test: 96.03% | Time: Fwd 1.7s, Bwd 1.8s, Upd 0.6s Total 4.1s
[MNIST-MorphReuse] Epoch 5/5: Train: 97.69%, Test: 96.70% | Time: Fwd 1.7s, Bwd 1.8s, Upd 0.6s Total 4.1s

MNIST-MorphReuse (Final) Memory (MB)
  Params:   1.20
  Grad:     0.32
  Buffers:  0.00
  Optim:    0.63
  Total:    2.15

Training LoRA:

MNIST-LoRA (Initial) Memory (MB)
  Params:   2.60
  Grad:     0.04
  Buffers:  0.00
  Optim:    0.09
  Total:    2.73
[MNIST-LoRA] Evaluating initial accuracy...
  Initial Train: 13.00%, Test: 12.43%
[MNIST-LoRA] Epoch 1/5: Train: 82.58%, Test: 82.65% | Time: Fwd 2.3s, Bwd 1.2s, Upd 0.4s Total 4.0s
[MNIST-LoRA] Epoch 2/5: Train: 86.27%, Test: 87.05% | Time: Fwd 2.4s, Bwd 1.2s, Upd 0.4s Total 4.0s
[MNIST-LoRA] Epoch 3/5: Train: 90.13%, Test: 90.02% | Time: Fwd 2.3s, Bwd 1.2s, Upd 0.4s Total 3.9s
[MNIST-LoRA] Epoch 4/5: Train: 91.38%, Test: 91.20% | Time: Fwd 2.3s, Bwd 1.2s, Upd 0.4s Total 3.9s
[MNIST-LoRA] Epoch 5/5: Train: 92.14%, Test: 91.73% | Time: Fwd 2.2s, Bwd 1.2s, Upd 0.4s Total 3.8s

MNIST-LoRA (Final) Memory (MB)
  Params:   2.60
  Grad:     0.04
  Buffers:  0.00
  Optim:    0.09
  Total:    2.73


==================================================
Running experiment: FashionMNIST
==================================================

Dataset: FashionMNIST
Type: Classification
Classes: 10
Input shape: (1, 28, 28)
Train samples: 60000
Test samples: 10000
Batch size: 128
Building Vision Baseline...
Building Vision MorphReuse...
Building Vision LoRA...

Model Parameters:
Baseline:
  Total params: 669,706
  Trainable params: 669,706
  Percentage trainable: 100.00%
MorphReuse:
  Total params: 315,713
  Trainable params: 83,009
  Percentage trainable: 26.29%
LoRA:
  Total params: 681,074
  Trainable params: 11,368
  Percentage trainable: 1.67%

Training Baseline:

FashionMNIST-Baseline (Initial) Memory (MB)
  Params:   2.55
  Grad:     2.55
  Buffers:  0.00
  Optim:    5.11
  Total:    10.22
[FashionMNIST-Baseline] Evaluating initial accuracy...
  Initial Train: 12.29%, Test: 12.41%
[FashionMNIST-Baseline] Epoch 1/5: Train: 85.46%, Test: 83.66% | Time: Fwd 1.8s, Bwd 2.3s, Upd 2.0s Total 6.1s
[FashionMNIST-Baseline] Epoch 2/5: Train: 87.77%, Test: 85.66% | Time: Fwd 1.8s, Bwd 2.3s, Upd 2.5s Total 6.5s
[FashionMNIST-Baseline] Epoch 3/5: Train: 88.21%, Test: 85.95% | Time: Fwd 1.8s, Bwd 2.3s, Upd 2.9s Total 6.9s
[FashionMNIST-Baseline] Epoch 4/5: Train: 88.68%, Test: 86.56% | Time: Fwd 1.8s, Bwd 2.2s, Upd 3.0s Total 7.0s
[FashionMNIST-Baseline] Epoch 5/5: Train: 90.89%, Test: 88.06% | Time: Fwd 1.8s, Bwd 2.2s, Upd 3.1s Total 7.2s

FashionMNIST-Baseline (Final) Memory (MB)
  Params:   2.55
  Grad:     2.55
  Buffers:  0.00
  Optim:    5.11
  Total:    10.22

Training MorphReuse:

FashionMNIST-MorphReuse (Initial) Memory (MB)
  Params:   1.20
  Grad:     0.32
  Buffers:  0.00
  Optim:    0.63
  Total:    2.15
[FashionMNIST-MorphReuse] Evaluating initial accuracy...
  Initial Train: 9.02%, Test: 8.45%
[FashionMNIST-MorphReuse] Epoch 1/5: Train: 84.44%, Test: 83.03% | Time: Fwd 1.7s, Bwd 1.8s, Upd 0.6s Total 4.1s
[FashionMNIST-MorphReuse] Epoch 2/5: Train: 86.14%, Test: 84.14% | Time: Fwd 1.7s, Bwd 1.8s, Upd 0.6s Total 4.1s
[FashionMNIST-MorphReuse] Epoch 3/5: Train: 87.78%, Test: 85.65% | Time: Fwd 1.8s, Bwd 1.8s, Upd 0.6s Total 4.2s
[FashionMNIST-MorphReuse] Epoch 4/5: Train: 88.34%, Test: 85.52% | Time: Fwd 1.7s, Bwd 1.8s, Upd 0.6s Total 4.1s
[FashionMNIST-MorphReuse] Epoch 5/5: Train: 89.10%, Test: 86.28% | Time: Fwd 1.7s, Bwd 1.8s, Upd 0.6s Total 4.1s

FashionMNIST-MorphReuse (Final) Memory (MB)
  Params:   1.20
  Grad:     0.32
  Buffers:  0.00
  Optim:    0.63
  Total:    2.15

Training LoRA:

FashionMNIST-LoRA (Initial) Memory (MB)
  Params:   2.60
  Grad:     0.04
  Buffers:  0.00
  Optim:    0.09
  Total:    2.73
[FashionMNIST-LoRA] Evaluating initial accuracy...
  Initial Train: 8.53%, Test: 8.50%
[FashionMNIST-LoRA] Epoch 1/5: Train: 77.46%, Test: 76.69% | Time: Fwd 2.3s, Bwd 1.2s, Upd 0.4s Total 3.8s
[FashionMNIST-LoRA] Epoch 2/5: Train: 80.25%, Test: 79.02% | Time: Fwd 2.3s, Bwd 1.2s, Upd 0.4s Total 4.0s
[FashionMNIST-LoRA] Epoch 3/5: Train: 82.44%, Test: 81.24% | Time: Fwd 2.3s, Bwd 1.2s, Upd 0.4s Total 4.0s
[FashionMNIST-LoRA] Epoch 4/5: Train: 83.77%, Test: 82.67% | Time: Fwd 2.4s, Bwd 1.3s, Upd 0.4s Total 4.2s
[FashionMNIST-LoRA] Epoch 5/5: Train: 84.53%, Test: 83.06% | Time: Fwd 2.3s, Bwd 1.2s, Upd 0.4s Total 4.0s

FashionMNIST-LoRA (Final) Memory (MB)
  Params:   2.60
  Grad:     0.04
  Buffers:  0.00
  Optim:    0.09
  Total:    2.73


==================================================
Running experiment: CIFAR10
==================================================

Dataset: CIFAR10
Type: Classification
Classes: 10
Input shape: (3, 32, 32)
Train samples: 50000
Test samples: 10000
Batch size: 128
Building Vision Baseline...
Building Vision MorphReuse...
Building Vision LoRA...

Model Parameters:
Baseline:
  Total params: 1,841,162
  Trainable params: 1,841,162
  Percentage trainable: 100.00%
MorphReuse:
  Total params: 608,577
  Trainable params: 83,009
  Percentage trainable: 13.64%
LoRA:
  Total params: 1,861,682
  Trainable params: 20,520
  Percentage trainable: 1.10%

Training Baseline:

CIFAR10-Baseline (Initial) Memory (MB)
  Params:   7.02
  Grad:     7.02
  Buffers:  0.00
  Optim:    14.05
  Total:    28.09
[CIFAR10-Baseline] Evaluating initial accuracy...
  Initial Train: 9.83%, Test: 9.71%
[CIFAR10-Baseline] Epoch 1/5: Train: 48.93%, Test: 46.88% | Time: Fwd 3.9s, Bwd 4.0s, Upd 4.0s Total 11.9s
[CIFAR10-Baseline] Epoch 2/5: Train: 55.63%, Test: 50.91% | Time: Fwd 3.7s, Bwd 3.8s, Upd 4.0s Total 11.5s
[CIFAR10-Baseline] Epoch 3/5: Train: 58.79%, Test: 51.64% | Time: Fwd 3.9s, Bwd 3.9s, Upd 4.4s Total 12.2s
[CIFAR10-Baseline] Epoch 4/5: Train: 63.06%, Test: 53.56% | Time: Fwd 4.0s, Bwd 4.0s, Upd 4.3s Total 12.2s
[CIFAR10-Baseline] Epoch 5/5: Train: 66.13%, Test: 53.73% | Time: Fwd 3.9s, Bwd 3.9s, Upd 4.3s Total 12.2s

CIFAR10-Baseline (Final) Memory (MB)
  Params:   7.02
  Grad:     7.02
  Buffers:  0.00
  Optim:    14.05
  Total:    28.09

Training MorphReuse:

CIFAR10-MorphReuse (Initial) Memory (MB)
  Params:   2.32
  Grad:     0.32
  Buffers:  0.00
  Optim:    0.63
  Total:    3.27
[CIFAR10-MorphReuse] Evaluating initial accuracy...
  Initial Train: 9.13%, Test: 9.13%
[CIFAR10-MorphReuse] Epoch 1/5: Train: 45.21%, Test: 43.46% | Time: Fwd 2.0s, Bwd 1.5s, Upd 0.5s Total 4.0s
[CIFAR10-MorphReuse] Epoch 2/5: Train: 48.82%, Test: 45.96% | Time: Fwd 2.2s, Bwd 1.5s, Upd 0.5s Total 4.2s
[CIFAR10-MorphReuse] Epoch 3/5: Train: 51.67%, Test: 47.56% | Time: Fwd 2.2s, Bwd 1.5s, Upd 0.5s Total 4.2s
[CIFAR10-MorphReuse] Epoch 4/5: Train: 54.08%, Test: 48.73% | Time: Fwd 2.1s, Bwd 1.4s, Upd 0.5s Total 4.0s
[CIFAR10-MorphReuse] Epoch 5/5: Train: 56.49%, Test: 49.35% | Time: Fwd 2.2s, Bwd 1.6s, Upd 0.5s Total 4.3s

CIFAR10-MorphReuse (Final) Memory (MB)
  Params:   2.32
  Grad:     0.32
  Buffers:  0.00
  Optim:    0.63
  Total:    3.27

Training LoRA:

CIFAR10-LoRA (Initial) Memory (MB)
  Params:   7.10
  Grad:     0.08
  Buffers:  0.00
  Optim:    0.16
  Total:    7.34
[CIFAR10-LoRA] Evaluating initial accuracy...
  Initial Train: 9.90%, Test: 10.22%
[CIFAR10-LoRA] Epoch 1/5: Train: 35.67%, Test: 35.84% | Time: Fwd 4.4s, Bwd 1.1s, Upd 0.4s Total 5.9s
[CIFAR10-LoRA] Epoch 2/5: Train: 37.78%, Test: 37.50% | Time: Fwd 4.3s, Bwd 1.1s, Upd 0.4s Total 5.8s
[CIFAR10-LoRA] Epoch 3/5: Train: 38.67%, Test: 38.73% | Time: Fwd 4.5s, Bwd 1.1s, Upd 0.4s Total 5.9s
[CIFAR10-LoRA] Epoch 4/5: Train: 40.63%, Test: 39.79% | Time: Fwd 4.5s, Bwd 1.1s, Upd 0.4s Total 6.0s
[CIFAR10-LoRA] Epoch 5/5: Train: 41.67%, Test: 41.01% | Time: Fwd 4.3s, Bwd 1.1s, Upd 0.4s Total 5.7s

CIFAR10-LoRA (Final) Memory (MB)
  Params:   7.10
  Grad:     0.08
  Buffers:  0.00
  Optim:    0.16
  Total:    7.34
